{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12472/12472 [==============================] - 57s 4ms/step - loss: 5.4381 - val_loss: 0.0228\n",
      "Epoch 2/100\n",
      "12472/12472 [==============================] - 59s 5ms/step - loss: 0.0579 - val_loss: 0.0018\n",
      "Epoch 3/100\n",
      "12472/12472 [==============================] - 67s 5ms/step - loss: 0.0422 - val_loss: 5.6072e-04\n",
      "Epoch 4/100\n",
      "12472/12472 [==============================] - 62s 5ms/step - loss: 0.0351 - val_loss: 7.0829e-04\n",
      "Epoch 5/100\n",
      "12472/12472 [==============================] - 55s 4ms/step - loss: 0.0301 - val_loss: 2.2851e-04\n",
      "Epoch 6/100\n",
      "12472/12472 [==============================] - 55s 4ms/step - loss: 0.0271 - val_loss: 4.9157e-04\n",
      "Epoch 7/100\n",
      "12472/12472 [==============================] - 55s 4ms/step - loss: 0.0250 - val_loss: 6.1757e-05\n",
      "Epoch 8/100\n",
      "12472/12472 [==============================] - 55s 4ms/step - loss: 0.0231 - val_loss: 3.1893e-04\n",
      "Epoch 9/100\n",
      "12472/12472 [==============================] - 55s 4ms/step - loss: 0.0217 - val_loss: 5.7638e-04\n",
      "Epoch 10/100\n",
      "12472/12472 [==============================] - 55s 4ms/step - loss: 0.0204 - val_loss: 7.8885e-05\n",
      "Epoch 11/100\n",
      "12472/12472 [==============================] - 55s 4ms/step - loss: 0.0191 - val_loss: 1.0240e-04\n",
      "Epoch 12/100\n",
      "12472/12472 [==============================] - 55s 4ms/step - loss: 0.0181 - val_loss: 5.4289e-04\n",
      "Epoch 13/100\n",
      "12472/12472 [==============================] - 56s 4ms/step - loss: 0.0174 - val_loss: 5.1178e-04\n",
      "Epoch 14/100\n",
      "12472/12472 [==============================] - 55s 4ms/step - loss: 0.0164 - val_loss: 6.7349e-04\n",
      "Epoch 15/100\n",
      "12472/12472 [==============================] - 55s 4ms/step - loss: 0.0154 - val_loss: 1.2171e-04\n",
      "Epoch 16/100\n",
      "12472/12472 [==============================] - 56s 5ms/step - loss: 0.0149 - val_loss: 9.0653e-04\n",
      "Epoch 17/100\n",
      "12472/12472 [==============================] - 53s 4ms/step - loss: 0.0142 - val_loss: 5.2314e-05\n",
      "Epoch 18/100\n",
      "12472/12472 [==============================] - 53s 4ms/step - loss: 0.0133 - val_loss: 0.0038\n",
      "Epoch 19/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0131 - val_loss: 0.0020\n",
      "Epoch 20/100\n",
      "12472/12472 [==============================] - 53s 4ms/step - loss: 0.0123 - val_loss: 4.1343e-04\n",
      "Epoch 21/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0119 - val_loss: 1.9450e-04\n",
      "Epoch 22/100\n",
      "12472/12472 [==============================] - 53s 4ms/step - loss: 0.0114 - val_loss: 7.1821e-05\n",
      "Epoch 23/100\n",
      "12472/12472 [==============================] - 53s 4ms/step - loss: 0.0108 - val_loss: 3.5501e-05\n",
      "Epoch 24/100\n",
      "12472/12472 [==============================] - 53s 4ms/step - loss: 0.0104 - val_loss: 4.9928e-04\n",
      "Epoch 25/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0104 - val_loss: 2.9558e-04\n",
      "Epoch 26/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0096 - val_loss: 0.0115\n",
      "Epoch 27/100\n",
      "12472/12472 [==============================] - 53s 4ms/step - loss: 0.0092 - val_loss: 3.5594e-04\n",
      "Epoch 28/100\n",
      "12472/12472 [==============================] - 53s 4ms/step - loss: 0.0089 - val_loss: 7.5413e-04\n",
      "Epoch 29/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0084 - val_loss: 8.1759e-05\n",
      "Epoch 30/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0085 - val_loss: 4.8729e-05\n",
      "Epoch 31/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0079 - val_loss: 0.0055\n",
      "Epoch 32/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0072 - val_loss: 0.0021\n",
      "Epoch 33/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0074 - val_loss: 0.0019\n",
      "Epoch 34/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0073 - val_loss: 0.0039\n",
      "Epoch 35/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0065 - val_loss: 0.0019\n",
      "Epoch 36/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0069 - val_loss: 9.4525e-05\n",
      "Epoch 37/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0061 - val_loss: 1.3892e-04\n",
      "Epoch 38/100\n",
      "12472/12472 [==============================] - 56s 5ms/step - loss: 0.0060 - val_loss: 1.7843e-04\n",
      "Epoch 39/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0059 - val_loss: 1.2081e-04\n",
      "Epoch 40/100\n",
      "12472/12472 [==============================] - 53s 4ms/step - loss: 0.0057 - val_loss: 1.2263e-04\n",
      "Epoch 41/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0053 - val_loss: 6.8846e-05\n",
      "Epoch 42/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0053 - val_loss: 2.7277e-04\n",
      "Epoch 43/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0052 - val_loss: 1.9430e-04\n",
      "Epoch 44/100\n",
      "12472/12472 [==============================] - 53s 4ms/step - loss: 0.0047 - val_loss: 0.0013\n",
      "Epoch 45/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0049 - val_loss: 4.1738e-05\n",
      "Epoch 46/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0045 - val_loss: 0.0012\n",
      "Epoch 47/100\n",
      "12472/12472 [==============================] - 53s 4ms/step - loss: 0.0041 - val_loss: 1.2588e-04\n",
      "Epoch 48/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0043 - val_loss: 9.5032e-05\n",
      "Epoch 49/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0042 - val_loss: 1.7643e-04\n",
      "Epoch 50/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0036 - val_loss: 3.0122e-04\n",
      "Epoch 51/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0040 - val_loss: 9.8044e-05\n",
      "Epoch 52/100\n",
      "12472/12472 [==============================] - 57s 5ms/step - loss: 0.0036 - val_loss: 0.0018\n",
      "Epoch 53/100\n",
      "12472/12472 [==============================] - 55s 4ms/step - loss: 0.0033 - val_loss: 1.0155e-04\n",
      "Epoch 54/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0040 - val_loss: 4.8732e-04\n",
      "Epoch 55/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0036 - val_loss: 1.7456e-04\n",
      "Epoch 56/100\n",
      "12472/12472 [==============================] - 53s 4ms/step - loss: 0.0029 - val_loss: 7.3250e-05\n",
      "Epoch 57/100\n",
      "12472/12472 [==============================] - 53s 4ms/step - loss: 0.0030 - val_loss: 0.0066\n",
      "Epoch 58/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0031 - val_loss: 6.5957e-04\n",
      "Epoch 59/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0029 - val_loss: 8.9994e-04\n",
      "Epoch 60/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0026 - val_loss: 1.4319e-05\n",
      "Epoch 61/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0028 - val_loss: 4.6984e-05\n",
      "Epoch 62/100\n",
      "12472/12472 [==============================] - 54s 4ms/step - loss: 0.0023 - val_loss: 0.0011\n",
      "Epoch 63/100\n",
      "12472/12472 [==============================] - 68s 5ms/step - loss: 0.0024 - val_loss: 7.5441e-04\n",
      "Epoch 64/100\n",
      "12472/12472 [==============================] - 62s 5ms/step - loss: 0.0024 - val_loss: 3.0145e-04\n",
      "Epoch 65/100\n",
      "12472/12472 [==============================] - 50s 4ms/step - loss: 0.0031 - val_loss: 1.8543e-04\n",
      "Epoch 66/100\n",
      "12472/12472 [==============================] - 62s 5ms/step - loss: 0.0019 - val_loss: 3.7974e-04\n",
      "Epoch 67/100\n",
      "12472/12472 [==============================] - 56s 4ms/step - loss: 0.0029 - val_loss: 1.9178e-04\n",
      "Epoch 68/100\n",
      "12472/12472 [==============================] - 56s 5ms/step - loss: 0.0018 - val_loss: 0.0108\n",
      "Epoch 69/100\n",
      "12472/12472 [==============================] - 57s 5ms/step - loss: 0.0034 - val_loss: 1.6604e-04\n",
      "Epoch 70/100\n",
      "12472/12472 [==============================] - 56s 4ms/step - loss: 0.0016 - val_loss: 4.4011e-04\n",
      "Epoch 71/100\n",
      "12472/12472 [==============================] - 56s 4ms/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 72/100\n",
      "12472/12472 [==============================] - 56s 4ms/step - loss: 0.0020 - val_loss: 4.3901e-05\n",
      "Epoch 73/100\n",
      "12472/12472 [==============================] - 57s 5ms/step - loss: 0.0015 - val_loss: 5.5131e-05\n",
      "Epoch 74/100\n",
      "12472/12472 [==============================] - 56s 5ms/step - loss: 0.0013 - val_loss: 6.9670e-05\n",
      "Epoch 75/100\n",
      "12472/12472 [==============================] - 57s 5ms/step - loss: 0.0014 - val_loss: 2.3821e-04\n",
      "Epoch 76/100\n",
      "12472/12472 [==============================] - 56s 4ms/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 77/100\n",
      "12472/12472 [==============================] - 56s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 78/100\n",
      "12472/12472 [==============================] - 56s 5ms/step - loss: 9.4589e-04 - val_loss: 0.0030\n",
      "Epoch 79/100\n",
      "12472/12472 [==============================] - 57s 5ms/step - loss: 0.0012 - val_loss: 3.5249e-04\n",
      "Epoch 80/100\n",
      "12472/12472 [==============================] - 57s 5ms/step - loss: 0.0010 - val_loss: 0.0157\n",
      "Epoch 81/100\n",
      "12472/12472 [==============================] - 58s 5ms/step - loss: 9.3926e-04 - val_loss: 1.8121e-05\n",
      "Epoch 82/100\n",
      "12472/12472 [==============================] - 58s 5ms/step - loss: 0.0011 - val_loss: 2.0830e-04\n",
      "Epoch 83/100\n",
      "12472/12472 [==============================] - 67s 5ms/step - loss: 8.9121e-04 - val_loss: 9.5761e-04\n",
      "Epoch 84/100\n",
      "12472/12472 [==============================] - 60s 5ms/step - loss: 0.0011 - val_loss: 1.5967e-04\n",
      "Epoch 85/100\n",
      "12472/12472 [==============================] - 59s 5ms/step - loss: 8.5277e-04 - val_loss: 8.3934e-04\n",
      "Epoch 86/100\n",
      "12472/12472 [==============================] - 59s 5ms/step - loss: 0.0025 - val_loss: 3.2076e-04\n",
      "Epoch 87/100\n",
      "12472/12472 [==============================] - 58s 5ms/step - loss: 7.6169e-04 - val_loss: 0.0016\n",
      "Epoch 88/100\n",
      "12472/12472 [==============================] - 58s 5ms/step - loss: 0.0015 - val_loss: 9.1547e-05\n",
      "Epoch 89/100\n",
      "12472/12472 [==============================] - 57s 5ms/step - loss: 7.5630e-04 - val_loss: 1.5266e-05\n",
      "Epoch 90/100\n",
      "12472/12472 [==============================] - 53s 4ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 91/100\n",
      "12472/12472 [==============================] - 57s 5ms/step - loss: 6.5102e-04 - val_loss: 2.1184e-04\n",
      "Epoch 92/100\n",
      "12472/12472 [==============================] - 53s 4ms/step - loss: 0.0018 - val_loss: 3.3595e-05\n",
      "Epoch 93/100\n",
      "12472/12472 [==============================] - 53s 4ms/step - loss: 6.8940e-04 - val_loss: 1.8836e-04\n",
      "Epoch 94/100\n",
      "12472/12472 [==============================] - 57s 5ms/step - loss: 0.0018 - val_loss: 3.4798e-05\n",
      "Epoch 95/100\n",
      "12472/12472 [==============================] - 59s 5ms/step - loss: 5.7546e-04 - val_loss: 0.0013\n",
      "Epoch 96/100\n",
      "12472/12472 [==============================] - 52s 4ms/step - loss: 0.0010 - val_loss: 2.5189e-05\n",
      "Epoch 97/100\n",
      "12472/12472 [==============================] - 52s 4ms/step - loss: 6.0164e-04 - val_loss: 4.9681e-05\n",
      "Epoch 98/100\n",
      "12472/12472 [==============================] - 47s 4ms/step - loss: 5.3268e-04 - val_loss: 5.3240e-05\n",
      "Epoch 99/100\n",
      "12472/12472 [==============================] - 48s 4ms/step - loss: 7.5977e-04 - val_loss: 5.7722e-05\n",
      "Epoch 100/100\n",
      "12472/12472 [==============================] - 48s 4ms/step - loss: 7.6089e-04 - val_loss: 4.2403e-05\n",
      "7795/7795 [==============================] - 12s 1ms/step\n",
      "Test MSE: 4.814785343475906e-05\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Activation, RepeatVector, Permute, Multiply, Lambda, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('Coffee_Stores_Data.csv')\n",
    "\n",
    "# Preprocess and remove NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Convert 'BusinessDate' to a datetime index\n",
    "df['BusinessDate'] = pd.to_datetime(df['BusinessDate'])\n",
    "df.set_index('BusinessDate', inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "df['Month'] = df.index.month\n",
    "df['Year'] = df.index.year\n",
    "df['Weekday'] = df.index.weekday\n",
    "df['Sales'] = df['SoldQuantity'] * 3  # Assuming a sales value to predict\n",
    "\n",
    "# Selecting features and target\n",
    "selected_features = ['SoldQuantity', 'ReceivedQuantity', 'LatestOrder', 'StockedOut']\n",
    "X = df[selected_features]\n",
    "y = df['Sales']\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Attention mechanism\n",
    "def attention_mechanism(inputs):\n",
    "    lstm_units = int(inputs.shape[2])\n",
    "    attention = Dense(1, activation='tanh')(inputs)\n",
    "    attention = Flatten()(attention)\n",
    "    attention = Activation('softmax')(attention)\n",
    "    attention = RepeatVector(lstm_units)(attention)\n",
    "    attention = Permute([2, 1])(attention)\n",
    "    attention_output = Multiply()([inputs, attention])\n",
    "    return attention_output\n",
    "\n",
    "# Define the model using the functional API\n",
    "inputs = Input(shape=(1, X_train.shape[2]))\n",
    "lstm_out = LSTM(50, return_sequences=True)(inputs)\n",
    "attention_out = attention_mechanism(lstm_out)\n",
    "lstm_out_2 = LSTM(50)(attention_out)\n",
    "output = Dense(1)(lstm_out_2)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[inputs], outputs=[output])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fit the model to the training data\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Test MSE: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I've added Bidirectional LSTM layers to capture patterns from both forward and backward states of the sequences.\n",
    "(This allows the network to capture dependencies in both forward and backward directions in the sequence data, potentially improving the model's learning capacity.)\n",
    "\n",
    "-Adjusted the batch size to 32 for more frequent updates.\n",
    "\n",
    "-Included an EarlyStopping callback to prevent overfitting by stopping the training if the validation loss does not improve for 10 epochs.\n",
    "\n",
    "-Changed the learning rate of the Adam optimizer to 0.001 for potentially better convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "24943/24943 [==============================] - 119s 4ms/step - loss: 27.9347 - val_loss: 23.7198\n",
      "Epoch 2/50\n",
      "24943/24943 [==============================] - 126s 5ms/step - loss: 23.7104 - val_loss: 22.8171\n",
      "Epoch 3/50\n",
      "24943/24943 [==============================] - 110s 4ms/step - loss: 23.3197 - val_loss: 22.7614\n",
      "Epoch 4/50\n",
      "24943/24943 [==============================] - 117s 5ms/step - loss: 23.1017 - val_loss: 22.5746\n",
      "Epoch 5/50\n",
      "24943/24943 [==============================] - 152s 6ms/step - loss: 22.9031 - val_loss: 22.7186\n",
      "Epoch 6/50\n",
      "24943/24943 [==============================] - 226s 9ms/step - loss: 22.7231 - val_loss: 22.2230\n",
      "Epoch 7/50\n",
      "24943/24943 [==============================] - 223s 9ms/step - loss: 22.6151 - val_loss: 22.2194\n",
      "Epoch 8/50\n",
      "24943/24943 [==============================] - 222s 9ms/step - loss: 22.5253 - val_loss: 22.0196\n",
      "Epoch 9/50\n",
      "24943/24943 [==============================] - 218s 9ms/step - loss: 22.4605 - val_loss: 21.9140\n",
      "Epoch 10/50\n",
      "24943/24943 [==============================] - 208s 8ms/step - loss: 22.4229 - val_loss: 22.0045\n",
      "Epoch 11/50\n",
      "24943/24943 [==============================] - 214s 9ms/step - loss: 22.3757 - val_loss: 21.9084\n",
      "Epoch 12/50\n",
      "24943/24943 [==============================] - 220s 9ms/step - loss: 22.3189 - val_loss: 21.7692\n",
      "Epoch 13/50\n",
      "24943/24943 [==============================] - 209s 8ms/step - loss: 22.2945 - val_loss: 22.0157\n",
      "Epoch 14/50\n",
      "24943/24943 [==============================] - 134s 5ms/step - loss: 22.2620 - val_loss: 21.7156\n",
      "Epoch 15/50\n",
      "24943/24943 [==============================] - 105s 4ms/step - loss: 22.2322 - val_loss: 21.9879\n",
      "Epoch 16/50\n",
      "24943/24943 [==============================] - 115s 5ms/step - loss: 22.1947 - val_loss: 21.6200\n",
      "Epoch 17/50\n",
      "24943/24943 [==============================] - 112s 4ms/step - loss: 22.1664 - val_loss: 21.6058\n",
      "Epoch 18/50\n",
      "24943/24943 [==============================] - 140s 6ms/step - loss: 22.1241 - val_loss: 21.6104\n",
      "Epoch 19/50\n",
      "24943/24943 [==============================] - 119s 5ms/step - loss: 22.1033 - val_loss: 21.6675\n",
      "Epoch 20/50\n",
      "24943/24943 [==============================] - 115s 5ms/step - loss: 22.0653 - val_loss: 21.5626\n",
      "Epoch 21/50\n",
      "24943/24943 [==============================] - 101s 4ms/step - loss: 22.0446 - val_loss: 21.9177\n",
      "Epoch 22/50\n",
      "24943/24943 [==============================] - 104s 4ms/step - loss: 22.0152 - val_loss: 21.5453\n",
      "Epoch 23/50\n",
      "24943/24943 [==============================] - 115s 5ms/step - loss: 21.9747 - val_loss: 21.6252\n",
      "Epoch 24/50\n",
      "24943/24943 [==============================] - 116s 5ms/step - loss: 21.9642 - val_loss: 21.4717\n",
      "Epoch 25/50\n",
      "24943/24943 [==============================] - 103s 4ms/step - loss: 21.9236 - val_loss: 21.5380\n",
      "Epoch 26/50\n",
      "24943/24943 [==============================] - 102s 4ms/step - loss: 21.9130 - val_loss: 21.6811\n",
      "Epoch 27/50\n",
      "24943/24943 [==============================] - 102s 4ms/step - loss: 21.8919 - val_loss: 21.5601\n",
      "Epoch 28/50\n",
      "24943/24943 [==============================] - 113s 5ms/step - loss: 21.8772 - val_loss: 21.4826\n",
      "Epoch 29/50\n",
      "24943/24943 [==============================] - 126s 5ms/step - loss: 21.8617 - val_loss: 21.5044\n",
      "Epoch 30/50\n",
      "24943/24943 [==============================] - 101s 4ms/step - loss: 21.8551 - val_loss: 21.4867\n",
      "Epoch 31/50\n",
      "24943/24943 [==============================] - 106s 4ms/step - loss: 21.8289 - val_loss: 21.8208\n",
      "Epoch 32/50\n",
      "24943/24943 [==============================] - 106s 4ms/step - loss: 21.8186 - val_loss: 21.4644\n",
      "Epoch 33/50\n",
      "24943/24943 [==============================] - 117s 5ms/step - loss: 21.8073 - val_loss: 21.6345\n",
      "Epoch 34/50\n",
      "24943/24943 [==============================] - 101s 4ms/step - loss: 21.7853 - val_loss: 21.3353\n",
      "Epoch 35/50\n",
      "24943/24943 [==============================] - 100s 4ms/step - loss: 21.7849 - val_loss: 21.7457\n",
      "Epoch 36/50\n",
      "24943/24943 [==============================] - 104s 4ms/step - loss: 21.7666 - val_loss: 21.4451\n",
      "Epoch 37/50\n",
      "24943/24943 [==============================] - 101s 4ms/step - loss: 21.7470 - val_loss: 21.7327\n",
      "Epoch 38/50\n",
      "24943/24943 [==============================] - 100s 4ms/step - loss: 21.7499 - val_loss: 21.3929\n",
      "Epoch 39/50\n",
      "24943/24943 [==============================] - 100s 4ms/step - loss: 21.7244 - val_loss: 21.3226\n",
      "Epoch 40/50\n",
      "24943/24943 [==============================] - 101s 4ms/step - loss: 21.7073 - val_loss: 21.5766\n",
      "Epoch 41/50\n",
      "24943/24943 [==============================] - 101s 4ms/step - loss: 21.7086 - val_loss: 21.4166\n",
      "Epoch 42/50\n",
      "24943/24943 [==============================] - 101s 4ms/step - loss: 21.6873 - val_loss: 21.3774\n",
      "Epoch 43/50\n",
      "24943/24943 [==============================] - 101s 4ms/step - loss: 21.6902 - val_loss: 21.3306\n",
      "Epoch 44/50\n",
      "24943/24943 [==============================] - 127s 5ms/step - loss: 21.6803 - val_loss: 21.3372\n",
      "Epoch 45/50\n",
      "24943/24943 [==============================] - 214s 9ms/step - loss: 21.6681 - val_loss: 21.3686\n",
      "Epoch 46/50\n",
      "24943/24943 [==============================] - 205s 8ms/step - loss: 21.6538 - val_loss: 21.3084\n",
      "Epoch 47/50\n",
      "24943/24943 [==============================] - 203s 8ms/step - loss: 21.6409 - val_loss: 21.4023\n",
      "Epoch 48/50\n",
      "24943/24943 [==============================] - 210s 8ms/step - loss: 21.6432 - val_loss: 21.2635\n",
      "Epoch 49/50\n",
      "24943/24943 [==============================] - 214s 9ms/step - loss: 21.6320 - val_loss: 21.5659\n",
      "Epoch 50/50\n",
      "24943/24943 [==============================] - 197s 8ms/step - loss: 21.6324 - val_loss: 21.3685\n",
      "7795/7795 [==============================] - 29s 3ms/step\n",
      "Test MSE: 21.67966904089208\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Activation, RepeatVector, Permute, Multiply, Lambda, Flatten, Bidirectional, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('Coffee_Stores_Data.csv')\n",
    "\n",
    "# Preprocess and remove NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Convert 'BusinessDate' to a datetime index\n",
    "df['BusinessDate'] = pd.to_datetime(df['BusinessDate'])\n",
    "df.set_index('BusinessDate', inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "df['Month'] = df.index.month\n",
    "df['Year'] = df.index.year\n",
    "\n",
    "df['Sales'] = df['SoldQuantity'] * 3  # Assuming a sales value to predict\n",
    "\n",
    "# Selecting features and target\n",
    "selected_features = ['ReceivedQuantity', 'LatestOrder', 'StockedOut', 'Month', 'Year']\n",
    "X = df[selected_features]\n",
    "y = df['Sales']\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Attention mechanism\n",
    "def attention_mechanism(inputs):\n",
    "    lstm_units = int(inputs.shape[2])\n",
    "    attention = Dense(1, activation='tanh')(inputs)\n",
    "    attention = Flatten()(attention)\n",
    "    attention = Activation('softmax')(attention)\n",
    "    attention = RepeatVector(lstm_units)(attention)\n",
    "    attention = Permute([2, 1])(attention)\n",
    "    attention_output = Multiply()([inputs, attention])\n",
    "    return attention_output\n",
    "\n",
    "# Define the model using the functional API\n",
    "inputs = Input(shape=(1, X_train.shape[2]))\n",
    "lstm_out = Bidirectional(LSTM(50, return_sequences=True))(inputs)\n",
    "attention_out = attention_mechanism(lstm_out)\n",
    "lstm_out_2 = Bidirectional(LSTM(50))(attention_out)\n",
    "output = Dense(1)(lstm_out_2)\n",
    "\n",
    "# Compile the model with adjustments in optimizer or loss\n",
    "model = Model(inputs=[inputs], outputs=[output])\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Add EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Fit the model with adjustments in epochs and batch_size\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Test MSE: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_array = X_test.reshape(-1, len(selected_features))\n",
    "\n",
    "# Convert y_test to a NumPy array and flatten if needed\n",
    "y_test_array = np.array(y_test).flatten()\n",
    "\n",
    "# Create a DataFrame with true values, predicted values, and selected features\n",
    "result_df = pd.DataFrame(X_test_array, columns=selected_features)\n",
    "result_df['True Sales'] = y_test_array\n",
    "result_df['Predicted Sales'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReceivedQuantity</th>\n",
       "      <th>LatestOrder</th>\n",
       "      <th>StockedOut</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>True Sales</th>\n",
       "      <th>Predicted Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.188119</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>34.675220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.138614</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.147985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.390342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.851048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.128713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.007157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249419</th>\n",
       "      <td>0.158416</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.172070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249420</th>\n",
       "      <td>0.133663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.012221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249421</th>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.824021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249422</th>\n",
       "      <td>0.128713</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.137159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249423</th>\n",
       "      <td>0.217822</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>36.059696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249424 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ReceivedQuantity  LatestOrder  StockedOut     Month  Year  True Sales  \\\n",
       "0               0.188119     0.608696         1.0  0.818182   0.0        33.0   \n",
       "1               0.138614     0.391304         0.0  0.090909   1.0         3.0   \n",
       "2               0.168317     0.652174         0.0  0.090909   1.0        12.0   \n",
       "3               0.148515     0.521739         0.0  0.909091   0.0         6.0   \n",
       "4               0.128713     0.000000         0.0  0.818182   0.0         0.0   \n",
       "...                  ...          ...         ...       ...   ...         ...   \n",
       "249419          0.158416     0.478261         1.0  0.545455   0.0        18.0   \n",
       "249420          0.133663     0.000000         0.0  0.636364   0.0         0.0   \n",
       "249421          0.168317     0.782609         0.0  0.636364   0.0        15.0   \n",
       "249422          0.128713     0.521739         1.0  0.272727   1.0         3.0   \n",
       "249423          0.217822     0.782609         0.0  0.727273   0.0        27.0   \n",
       "\n",
       "        Predicted Sales  \n",
       "0             34.675220  \n",
       "1              4.147985  \n",
       "2             15.390342  \n",
       "3              6.851048  \n",
       "4             -0.007157  \n",
       "...                 ...  \n",
       "249419        17.172070  \n",
       "249420        -0.012221  \n",
       "249421        15.824021  \n",
       "249422         6.137159  \n",
       "249423        36.059696  \n",
       "\n",
       "[249424 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout in LSTM Layers\n",
    "Dropout has been added to the LSTM layers (dropout=0.2). Dropout is a regularization technique where randomly selected neurons are ignored during training, which helps in preventing overfitting.\n",
    "\n",
    "ReduceLROnPlateau Callback: The ReduceLROnPlateau callback is added along with EarlyStopping. This callback reduces the learning rate when a metric has stopped improving. In this case, it monitors the validation loss ('val_loss'), reducing the learning rate by a factor of 0.2 if there's no improvement in 5 epochs, with a minimum learning rate set to 0.0001.\n",
    "\n",
    "#Changed Training Parameters: The training parameters have been adjusted, specifically the number of epochs is reduced to 20 (from 50 in the first code), and the batch size is increased to 64 (from 32).##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12472/12472 [==============================] - 90s 7ms/step - loss: 50.1681 - val_loss: 78.9875 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "12472/12472 [==============================] - 74s 6ms/step - loss: 30.4708 - val_loss: 87.6903 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "12472/12472 [==============================] - 64s 5ms/step - loss: 28.1976 - val_loss: 79.2058 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "12472/12472 [==============================] - 74s 6ms/step - loss: 27.6304 - val_loss: 79.5329 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "12472/12472 [==============================] - 72s 6ms/step - loss: 27.5418 - val_loss: 75.3420 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "12472/12472 [==============================] - 69s 6ms/step - loss: 27.4514 - val_loss: 79.7319 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "12472/12472 [==============================] - 73s 6ms/step - loss: 27.2753 - val_loss: 80.6127 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "12472/12472 [==============================] - 78s 6ms/step - loss: 27.0881 - val_loss: 84.3497 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "12472/12472 [==============================] - 83s 7ms/step - loss: 27.0102 - val_loss: 76.7294 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "12472/12472 [==============================] - 83s 7ms/step - loss: 26.9605 - val_loss: 79.7349 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "12472/12472 [==============================] - 83s 7ms/step - loss: 26.6922 - val_loss: 79.8898 - lr: 2.0000e-04\n",
      "Epoch 12/20\n",
      "12472/12472 [==============================] - 83s 7ms/step - loss: 26.6701 - val_loss: 79.8246 - lr: 2.0000e-04\n",
      "Epoch 13/20\n",
      "12472/12472 [==============================] - 71s 6ms/step - loss: 26.6113 - val_loss: 80.3434 - lr: 2.0000e-04\n",
      "Epoch 14/20\n",
      "12472/12472 [==============================] - 70s 6ms/step - loss: 26.5967 - val_loss: 81.1183 - lr: 2.0000e-04\n",
      "Epoch 15/20\n",
      "12472/12472 [==============================] - 68s 5ms/step - loss: 26.4862 - val_loss: 80.4869 - lr: 2.0000e-04\n",
      "7795/7795 [==============================] - 17s 2ms/step\n",
      "Test MSE: 76.37539752894307\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Activation, RepeatVector, Permute, Multiply, Lambda, Flatten, Bidirectional, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('Coffee_Stores_Data.csv')\n",
    "\n",
    "# Preprocess and remove NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Convert 'BusinessDate' to a datetime index\n",
    "df['BusinessDate'] = pd.to_datetime(df['BusinessDate'])\n",
    "df.set_index('BusinessDate', inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "df['Month'] = df.index.month\n",
    "df['Year'] = df.index.year\n",
    "df['Sales'] = df['SoldQuantity'] * 3  # Assuming a sales value to predict\n",
    "\n",
    "# Selecting features and target\n",
    "selected_features = ['ReceivedQuantity', 'LatestOrder', 'StockedOut', 'Month', 'Year']\n",
    "X = df[selected_features]\n",
    "y = df['Sales']\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Attention mechanism\n",
    "def attention_mechanism(inputs):\n",
    "    lstm_units = int(inputs.shape[2])\n",
    "    attention = Dense(1, activation='tanh')(inputs)\n",
    "    attention = Flatten()(attention)\n",
    "    attention = Activation('softmax')(attention)\n",
    "    attention = RepeatVector(lstm_units)(attention)\n",
    "    attention = Permute([2, 1])(attention)\n",
    "    attention_output = Multiply()([inputs, attention])\n",
    "    return attention_output\n",
    "\n",
    "# Define the model using the functional API with added dropout\n",
    "inputs = Input(shape=(1, X_train.shape[2]))\n",
    "lstm_out = Bidirectional(LSTM(50, return_sequences=True, dropout=0.2))(inputs)  # Added dropout\n",
    "attention_out = attention_mechanism(lstm_out)\n",
    "lstm_out_2 = Bidirectional(LSTM(50, dropout=0.2))(attention_out)  # Added dropout\n",
    "output = Dense(1)(lstm_out_2)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[inputs], outputs=[output])\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Add EarlyStopping and ReduceLROnPlateau\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2, verbose=1, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Test MSE: {mse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features added : \n",
    "Increased LSTM Units: The units in the first Bidirectional LSTM layer are increased to 64 (from 50 in Code 1), and a second Bidirectional LSTM layer with 32 units is added.\n",
    "\n",
    "Dense Layer Configuration Change: A Dense layer with 64 units and 'relu' activation is introduced before the final output layer, without the attention mechanism used in Code 1.\n",
    "\n",
    "Change in Dropout Rate: The dropout rate in the first Bidirectional LSTM layer is set to 0.1 (compared to the 0.2 rate in Code 1 for dropout layers).\n",
    "\n",
    "Different Optimizer: The optimizer is changed from Adam to Nadam ('nadam').\n",
    "\n",
    "Adjustment in Early Stopping and ReduceLROnPlateau Callbacks:\n",
    "\n",
    "EarlyStopping: The patience parameter is increased to 15 (from 10 in Code 1).\n",
    "ReduceLROnPlateau: The factor is set to 0.1, patience to 10, and minimum learning rate to 0.00001.\n",
    "Adjusted Training Parameters: The number of epochs is increased to 100 (from 50), the batch size is increased to 64 (from 32), and the validation split is increased to 0.25 (from 0.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11692/11692 [==============================] - 64s 5ms/step - loss: 34.5741 - val_loss: 51.6279 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "11692/11692 [==============================] - 54s 5ms/step - loss: 25.2602 - val_loss: 50.2780 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "11692/11692 [==============================] - 57s 5ms/step - loss: 24.7781 - val_loss: 49.2713 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "11692/11692 [==============================] - 57s 5ms/step - loss: 24.5946 - val_loss: 48.1954 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "11692/11692 [==============================] - 57s 5ms/step - loss: 24.4321 - val_loss: 53.0323 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "11692/11692 [==============================] - 56s 5ms/step - loss: 24.3482 - val_loss: 50.5583 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "11692/11692 [==============================] - 56s 5ms/step - loss: 24.3591 - val_loss: 44.7952 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "11692/11692 [==============================] - 57s 5ms/step - loss: 24.1998 - val_loss: 51.6329 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "11692/11692 [==============================] - 57s 5ms/step - loss: 24.1268 - val_loss: 48.0375 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "11692/11692 [==============================] - 56s 5ms/step - loss: 24.0893 - val_loss: 48.5898 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "11692/11692 [==============================] - 58s 5ms/step - loss: 23.9868 - val_loss: 49.2707 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "11692/11692 [==============================] - 59s 5ms/step - loss: 23.9594 - val_loss: 51.3155 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "11692/11692 [==============================] - 59s 5ms/step - loss: 23.8290 - val_loss: 50.1902 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "11692/11692 [==============================] - 58s 5ms/step - loss: 23.8241 - val_loss: 45.5120 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "11692/11692 [==============================] - 59s 5ms/step - loss: 23.8267 - val_loss: 50.3218 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "11692/11692 [==============================] - 59s 5ms/step - loss: 23.7950 - val_loss: 46.2462 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "11692/11692 [==============================] - 59s 5ms/step - loss: 23.7214 - val_loss: 46.9565 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "11692/11692 [==============================] - 58s 5ms/step - loss: 23.3650 - val_loss: 47.8970 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "11692/11692 [==============================] - 58s 5ms/step - loss: 23.3057 - val_loss: 49.1058 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "11692/11692 [==============================] - 58s 5ms/step - loss: 23.3645 - val_loss: 47.8397 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "11692/11692 [==============================] - 58s 5ms/step - loss: 23.3360 - val_loss: 48.7226 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "11692/11692 [==============================] - 58s 5ms/step - loss: 23.3274 - val_loss: 47.9969 - lr: 1.0000e-04\n",
      "7795/7795 [==============================] - 15s 2ms/step\n",
      "Test MSE: 45.28084127201594\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('Coffee_Stores_Data.csv')\n",
    "\n",
    "# Preprocess and remove NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Convert 'BusinessDate' to a datetime index and perform feature engineering\n",
    "df['BusinessDate'] = pd.to_datetime(df['BusinessDate'])\n",
    "df.set_index('BusinessDate', inplace=True)\n",
    "df['Month'] = df.index.month\n",
    "df['Year'] = df.index.year\n",
    "df['Sales'] = df['SoldQuantity'] * 3  # Assuming a sales value to predict\n",
    "\n",
    "# Selecting features and target\n",
    "selected_features = ['ReceivedQuantity', 'LatestOrder', 'StockedOut', 'Month', 'Year']\n",
    "X = df[selected_features]\n",
    "y = df['Sales']\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Define a more complex LSTM model with attention mechanism\n",
    "inputs = Input(shape=(1, X_train.shape[2]))\n",
    "x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.1))(inputs)\n",
    "x = Bidirectional(LSTM(32, return_sequences=True))(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "output = Dense(1)(x)\n",
    "\n",
    "# Compile the model with a different optimizer\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer='nadam', loss='mean_squared_error')\n",
    "\n",
    "# Add EarlyStopping and ReduceLROnPlateau\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=0.00001)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_split=0.25, verbose=1, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Predictions and Evaluation\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Test MSE: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This model didnt perfome well\n",
    "\n",
    "#Complexity of the Model: The increased complexity (more LSTM units and layers) might not have been necessary for this dataset, potentially leading to overfitting or inefficient learning.\n",
    "\n",
    "\n",
    "#Removal of Attention Mechanism: The absence of the attention mechanism, which was present in Code 1, might have reduced the model's ability to focus on important features in the sequence data.\n",
    "\n",
    "#Change in Optimizer: The switch to Nadam might not have been as effective as Adam for this particular problem and dataset.\n",
    "\n",
    "#Training Dynamics: The increased number of epochs and changes in batch size and learning rate might have influenced the training, but not in a way that benefits the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReceivedQuantity</th>\n",
       "      <th>LatestOrder</th>\n",
       "      <th>StockedOut</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>True Sales</th>\n",
       "      <th>Predicted Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.188119</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>24.541569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.138614</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.801142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.632602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.307131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.128713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.576154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249419</th>\n",
       "      <td>0.158416</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>8.765635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249420</th>\n",
       "      <td>0.133663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.602673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249421</th>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.479501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249422</th>\n",
       "      <td>0.128713</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.067169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249423</th>\n",
       "      <td>0.217822</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.020144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249424 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ReceivedQuantity  LatestOrder  StockedOut     Month  Year  True Sales  \\\n",
       "0               0.188119     0.608696         1.0  0.818182   0.0        33.0   \n",
       "1               0.138614     0.391304         0.0  0.090909   1.0         3.0   \n",
       "2               0.168317     0.652174         0.0  0.090909   1.0        12.0   \n",
       "3               0.148515     0.521739         0.0  0.909091   0.0         6.0   \n",
       "4               0.128713     0.000000         0.0  0.818182   0.0         0.0   \n",
       "...                  ...          ...         ...       ...   ...         ...   \n",
       "249419          0.158416     0.478261         1.0  0.545455   0.0        18.0   \n",
       "249420          0.133663     0.000000         0.0  0.636364   0.0         0.0   \n",
       "249421          0.168317     0.782609         0.0  0.636364   0.0        15.0   \n",
       "249422          0.128713     0.521739         1.0  0.272727   1.0         3.0   \n",
       "249423          0.217822     0.782609         0.0  0.727273   0.0        27.0   \n",
       "\n",
       "        Predicted Sales  \n",
       "0             24.541569  \n",
       "1              4.801142  \n",
       "2              9.632602  \n",
       "3              4.307131  \n",
       "4              0.576154  \n",
       "...                 ...  \n",
       "249419         8.765635  \n",
       "249420         0.602673  \n",
       "249421         9.479501  \n",
       "249422        11.067169  \n",
       "249423        27.020144  \n",
       "\n",
       "[249424 rows x 7 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_array = X_test.reshape(-1, len(selected_features))\n",
    "\n",
    "# Convert y_test to a NumPy array and flatten if needed\n",
    "y_test_array = np.array(y_test).flatten()\n",
    "\n",
    "# Create a DataFrame with true values, predicted values, and selected features\n",
    "result_df = pd.DataFrame(X_test_array, columns=selected_features)\n",
    "result_df['True Sales'] = y_test_array\n",
    "result_df['Predicted Sales'] = predictions\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes in LSTM Layers Configuration:\n",
    "\n",
    "Increased number of units in the first Bidirectional LSTM layer from 50 to 64.\n",
    "Added a second Bidirectional LSTM layer with 32 units.\n",
    "Increased dropout rate in the first LSTM layer from 0.2 to 0.15.\n",
    "Additional Dropout Layer: An extra dropout layer with a rate of 0.15 is added after the second LSTM layer.\n",
    "\n",
    "Dense Layer Before Output: A Dense layer with 64 units and 'relu' activation is introduced before the final output layer. This is a change from the attention mechanism used in Code 1.\n",
    "\n",
    "Changed Optimizer and Learning Rate: The optimizer is changed from Adam to Nadam, with an adjusted learning rate of 0.0005.\n",
    "\n",
    "Modification in Callbacks:\n",
    "\n",
    "EarlyStopping: Increased patience from 10 to 20.\n",
    "ReduceLROnPlateau: Adjusted the parameters, including the factor to 0.1, patience to 7, and minimum learning rate to 0.00001.\n",
    "Adjustment in Training Parameters: Increased the batch size to 128 and validation split to 0.25, while keeping the number of epochs at 50.##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5846/5846 [==============================] - 47s 7ms/step - loss: 55.1928 - val_loss: 96.2659 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "5846/5846 [==============================] - 38s 6ms/step - loss: 31.1717 - val_loss: 145.1408 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "5846/5846 [==============================] - 36s 6ms/step - loss: 28.0635 - val_loss: 118.0420 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "5846/5846 [==============================] - 37s 6ms/step - loss: 27.2843 - val_loss: 113.2483 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "5846/5846 [==============================] - 36s 6ms/step - loss: 26.8590 - val_loss: 99.4146 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "5846/5846 [==============================] - 37s 6ms/step - loss: 26.5588 - val_loss: 103.1740 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "5846/5846 [==============================] - 39s 7ms/step - loss: 26.4659 - val_loss: 95.9484 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "5846/5846 [==============================] - 38s 7ms/step - loss: 26.3416 - val_loss: 91.9516 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "5846/5846 [==============================] - 37s 6ms/step - loss: 26.2274 - val_loss: 89.2026 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "5846/5846 [==============================] - 38s 6ms/step - loss: 26.1392 - val_loss: 86.9138 - lr: 5.0000e-04\n",
      "Epoch 11/50\n",
      "5846/5846 [==============================] - 37s 6ms/step - loss: 26.0927 - val_loss: 79.9687 - lr: 5.0000e-04\n",
      "Epoch 12/50\n",
      "5846/5846 [==============================] - 39s 7ms/step - loss: 25.9500 - val_loss: 83.5216 - lr: 5.0000e-04\n",
      "Epoch 13/50\n",
      "5846/5846 [==============================] - 39s 7ms/step - loss: 25.9717 - val_loss: 79.6359 - lr: 5.0000e-04\n",
      "Epoch 14/50\n",
      "5846/5846 [==============================] - 42s 7ms/step - loss: 25.9079 - val_loss: 74.6211 - lr: 5.0000e-04\n",
      "Epoch 15/50\n",
      "5846/5846 [==============================] - 37s 6ms/step - loss: 25.8417 - val_loss: 77.3438 - lr: 5.0000e-04\n",
      "Epoch 16/50\n",
      "5846/5846 [==============================] - 36s 6ms/step - loss: 25.8739 - val_loss: 76.7321 - lr: 5.0000e-04\n",
      "Epoch 17/50\n",
      "5846/5846 [==============================] - 37s 6ms/step - loss: 25.8526 - val_loss: 73.7527 - lr: 5.0000e-04\n",
      "Epoch 18/50\n",
      "5846/5846 [==============================] - 37s 6ms/step - loss: 25.7161 - val_loss: 75.2933 - lr: 5.0000e-04\n",
      "Epoch 19/50\n",
      "5846/5846 [==============================] - 37s 6ms/step - loss: 25.7409 - val_loss: 74.8879 - lr: 5.0000e-04\n",
      "Epoch 20/50\n",
      "5846/5846 [==============================] - 38s 6ms/step - loss: 25.6897 - val_loss: 74.6371 - lr: 5.0000e-04\n",
      "Epoch 21/50\n",
      "5846/5846 [==============================] - 38s 6ms/step - loss: 25.6188 - val_loss: 70.8115 - lr: 5.0000e-04\n",
      "Epoch 22/50\n",
      "5846/5846 [==============================] - 39s 7ms/step - loss: 25.7539 - val_loss: 74.0077 - lr: 5.0000e-04\n",
      "Epoch 23/50\n",
      "5846/5846 [==============================] - 37s 6ms/step - loss: 25.6562 - val_loss: 72.9471 - lr: 5.0000e-04\n",
      "Epoch 24/50\n",
      "5846/5846 [==============================] - 37s 6ms/step - loss: 25.6027 - val_loss: 69.7432 - lr: 5.0000e-04\n",
      "Epoch 25/50\n",
      "5846/5846 [==============================] - 39s 7ms/step - loss: 25.5466 - val_loss: 71.8622 - lr: 5.0000e-04\n",
      "Epoch 26/50\n",
      "5846/5846 [==============================] - 48s 8ms/step - loss: 25.5515 - val_loss: 70.7457 - lr: 5.0000e-04\n",
      "Epoch 27/50\n",
      "5846/5846 [==============================] - 72s 12ms/step - loss: 25.6029 - val_loss: 71.2846 - lr: 5.0000e-04\n",
      "Epoch 28/50\n",
      "5846/5846 [==============================] - 74s 13ms/step - loss: 25.5438 - val_loss: 72.2657 - lr: 5.0000e-04\n",
      "Epoch 29/50\n",
      "5846/5846 [==============================] - 76s 13ms/step - loss: 25.4511 - val_loss: 69.8801 - lr: 5.0000e-04\n",
      "Epoch 30/50\n",
      "5846/5846 [==============================] - 75s 13ms/step - loss: 25.4350 - val_loss: 71.7536 - lr: 5.0000e-04\n",
      "Epoch 31/50\n",
      "5846/5846 [==============================] - 75s 13ms/step - loss: 25.4781 - val_loss: 70.3258 - lr: 5.0000e-04\n",
      "Epoch 32/50\n",
      "5846/5846 [==============================] - 75s 13ms/step - loss: 25.2359 - val_loss: 71.1685 - lr: 5.0000e-05\n",
      "Epoch 33/50\n",
      "5846/5846 [==============================] - 74s 13ms/step - loss: 25.2413 - val_loss: 71.4965 - lr: 5.0000e-05\n",
      "Epoch 34/50\n",
      "5846/5846 [==============================] - 74s 13ms/step - loss: 25.2071 - val_loss: 71.8997 - lr: 5.0000e-05\n",
      "Epoch 35/50\n",
      "5846/5846 [==============================] - 74s 13ms/step - loss: 25.2014 - val_loss: 71.7799 - lr: 5.0000e-05\n",
      "Epoch 36/50\n",
      "5846/5846 [==============================] - 78s 13ms/step - loss: 25.2135 - val_loss: 72.6172 - lr: 5.0000e-05\n",
      "Epoch 37/50\n",
      "5846/5846 [==============================] - 73s 13ms/step - loss: 25.2375 - val_loss: 71.9700 - lr: 5.0000e-05\n",
      "Epoch 38/50\n",
      "5846/5846 [==============================] - 71s 12ms/step - loss: 25.1556 - val_loss: 72.7592 - lr: 5.0000e-05\n",
      "Epoch 39/50\n",
      "5846/5846 [==============================] - 83s 14ms/step - loss: 25.1573 - val_loss: 72.9055 - lr: 1.0000e-05\n",
      "Epoch 40/50\n",
      "5846/5846 [==============================] - 77s 13ms/step - loss: 25.2312 - val_loss: 72.4647 - lr: 1.0000e-05\n",
      "Epoch 41/50\n",
      "5846/5846 [==============================] - 76s 13ms/step - loss: 25.1715 - val_loss: 72.0674 - lr: 1.0000e-05\n",
      "Epoch 42/50\n",
      "5846/5846 [==============================] - 77s 13ms/step - loss: 25.2177 - val_loss: 72.7600 - lr: 1.0000e-05\n",
      "Epoch 43/50\n",
      "5846/5846 [==============================] - 73s 12ms/step - loss: 25.1691 - val_loss: 72.6928 - lr: 1.0000e-05\n",
      "Epoch 44/50\n",
      "5846/5846 [==============================] - 74s 13ms/step - loss: 25.2253 - val_loss: 72.8198 - lr: 1.0000e-05\n",
      "7795/7795 [==============================] - 28s 3ms/step\n",
      "Test MSE: 70.37653780485536\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('Coffee_Stores_Data.csv')\n",
    "\n",
    "# Preprocess and remove NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Convert 'BusinessDate' to a datetime index and perform feature engineering\n",
    "df['BusinessDate'] = pd.to_datetime(df['BusinessDate'])\n",
    "df.set_index('BusinessDate', inplace=True)\n",
    "df['Month'] = df.index.month\n",
    "df['Year'] = df.index.year\n",
    "df['Sales'] = df['SoldQuantity'] * 3  # Assuming a sales value to predict\n",
    "\n",
    "# Selecting features and target\n",
    "selected_features = ['ReceivedQuantity', 'LatestOrder', 'StockedOut', 'Month', 'Year']\n",
    "X = df[selected_features]\n",
    "y = df['Sales']\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Define a more complex LSTM model with attention mechanism\n",
    "inputs = Input(shape=(1, X_train.shape[2]))\n",
    "x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.15))(inputs)  # Increased dropout\n",
    "x = Bidirectional(LSTM(32, return_sequences=True))(x)\n",
    "x = Dropout(0.15)(x)  # Additional dropout layer\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "output = Dense(1)(x)\n",
    "\n",
    "# Compile the model with a different optimizer and learning rate\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer=Nadam(learning_rate=0.0005), loss='mean_squared_error')  # Adjusted learning rate\n",
    "\n",
    "# Add EarlyStopping and ReduceLROnPlateau\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)  # Increased patience\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, min_lr=0.00001)  # Adjusted parameters\n",
    "\n",
    "# Fit the model with an adjusted batch size\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=128, validation_split=0.25, verbose=1, callbacks=[early_stopping, reduce_lr])  # Adjusted batch size\n",
    "\n",
    "# Predictions and Evaluation\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Test MSE: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It basic lstm + attention model had performed better \n",
    "#Increased Model Complexity: The additional LSTM layer and Dense layer might have made the model too complex for the data, potentially leading to overfitting despite the increased dropout rates.\n",
    "#Optimizer and Learning Rate Changes: The use of Nadam with a lower learning rate might not have been as effective as the original configuration in Code 1.\n",
    "#Training Parameters: The increased batch size and validation split could have impacted the training dynamics, potentially leading to less effective learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes in lstm \n",
    "Conv1D Layer: A Conv1D layer is introduced at the beginning of the model. Conv1D layers are used for extracting features from sequences, which can be effective in understanding temporal patterns.\n",
    "\n",
    "Increased LSTM Units and Additional Dropout: The number of units in the first Bidirectional LSTM layer is increased to 80, and in the second layer to 40. Additionally, the dropout rate is set to 0.25 in both LSTM layers to help prevent overfitting.\n",
    "\n",
    "Dense Layer with Regularization: A Dense layer with 100 units and 'relu' activation is added, which also includes L1 and L2 regularization (kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)). Regularization is a technique to reduce overfitting by penalizing large weights.\n",
    "\n",
    "Batch Normalization: A Batch Normalization layer is added, which can help in stabilizing and speeding up the training process by normalizing the input layer by re-centering and re-scaling.\n",
    "\n",
    "Increased Dropout Rate: The dropout rate after the Dense layer is increased to 0.3.\n",
    "\n",
    "Changed Optimizer, Learning Rate, and Early Stopping Parameters:\n",
    "\n",
    "The optimizer is changed to Nadam with a learning rate of 0.0005.\n",
    "The EarlyStopping patience is increased to 30, and ReduceLROnPlateau patience to 10, with a minimum learning rate of 0.00001.\n",
    "Adjusted Training Parameters: The number of epochs is substantially increased to 350, the batch size is set to 64, and the validation split is increased to 0.3.##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "10913/10913 [==============================] - 107s 9ms/step - loss: 35.3001 - val_loss: 24.0305 - lr: 5.0000e-04\n",
      "Epoch 2/350\n",
      "10913/10913 [==============================] - 78s 7ms/step - loss: 27.6431 - val_loss: 28.8501 - lr: 5.0000e-04\n",
      "Epoch 3/350\n",
      "10913/10913 [==============================] - 78s 7ms/step - loss: 26.8610 - val_loss: 23.8886 - lr: 5.0000e-04\n",
      "Epoch 4/350\n",
      "10913/10913 [==============================] - 78s 7ms/step - loss: 26.7226 - val_loss: 25.8867 - lr: 5.0000e-04\n",
      "Epoch 5/350\n",
      "10913/10913 [==============================] - 81s 7ms/step - loss: 26.4597 - val_loss: 23.5229 - lr: 5.0000e-04\n",
      "Epoch 6/350\n",
      "10913/10913 [==============================] - 79s 7ms/step - loss: 26.1609 - val_loss: 24.0493 - lr: 5.0000e-04\n",
      "Epoch 7/350\n",
      "10913/10913 [==============================] - 79s 7ms/step - loss: 26.1188 - val_loss: 23.5032 - lr: 5.0000e-04\n",
      "Epoch 8/350\n",
      "10913/10913 [==============================] - 79s 7ms/step - loss: 26.0336 - val_loss: 23.1326 - lr: 5.0000e-04\n",
      "Epoch 9/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 25.9546 - val_loss: 23.2424 - lr: 5.0000e-04\n",
      "Epoch 10/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 25.8611 - val_loss: 23.2012 - lr: 5.0000e-04\n",
      "Epoch 11/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 25.8482 - val_loss: 22.9375 - lr: 5.0000e-04\n",
      "Epoch 12/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 25.7312 - val_loss: 22.7372 - lr: 5.0000e-04\n",
      "Epoch 13/350\n",
      "10913/10913 [==============================] - 81s 7ms/step - loss: 25.7068 - val_loss: 23.9056 - lr: 5.0000e-04\n",
      "Epoch 14/350\n",
      "10913/10913 [==============================] - 81s 7ms/step - loss: 25.7447 - val_loss: 22.9130 - lr: 5.0000e-04\n",
      "Epoch 15/350\n",
      "10913/10913 [==============================] - 81s 7ms/step - loss: 25.7461 - val_loss: 23.2288 - lr: 5.0000e-04\n",
      "Epoch 16/350\n",
      "10913/10913 [==============================] - 82s 7ms/step - loss: 25.6818 - val_loss: 23.4413 - lr: 5.0000e-04\n",
      "Epoch 17/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 25.5427 - val_loss: 23.1874 - lr: 5.0000e-04\n",
      "Epoch 18/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 25.5089 - val_loss: 22.8576 - lr: 5.0000e-04\n",
      "Epoch 19/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 25.5214 - val_loss: 23.1649 - lr: 5.0000e-04\n",
      "Epoch 20/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 25.4029 - val_loss: 22.8947 - lr: 5.0000e-04\n",
      "Epoch 21/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 25.4017 - val_loss: 22.7811 - lr: 5.0000e-04\n",
      "Epoch 22/350\n",
      "10913/10913 [==============================] - 81s 7ms/step - loss: 25.2901 - val_loss: 23.2107 - lr: 5.0000e-04\n",
      "Epoch 23/350\n",
      "10913/10913 [==============================] - 81s 7ms/step - loss: 24.8337 - val_loss: 22.5410 - lr: 5.0000e-05\n",
      "Epoch 24/350\n",
      "10913/10913 [==============================] - 83s 8ms/step - loss: 24.8137 - val_loss: 22.5070 - lr: 5.0000e-05\n",
      "Epoch 25/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.7786 - val_loss: 22.4280 - lr: 5.0000e-05\n",
      "Epoch 26/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.7134 - val_loss: 22.4495 - lr: 5.0000e-05\n",
      "Epoch 27/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.7247 - val_loss: 22.3913 - lr: 5.0000e-05\n",
      "Epoch 28/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.7385 - val_loss: 22.4810 - lr: 5.0000e-05\n",
      "Epoch 29/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.7250 - val_loss: 22.4192 - lr: 5.0000e-05\n",
      "Epoch 30/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.7416 - val_loss: 22.4208 - lr: 5.0000e-05\n",
      "Epoch 31/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.7015 - val_loss: 22.3518 - lr: 5.0000e-05\n",
      "Epoch 32/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.7142 - val_loss: 22.4415 - lr: 5.0000e-05\n",
      "Epoch 33/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.6746 - val_loss: 22.4097 - lr: 5.0000e-05\n",
      "Epoch 34/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.7445 - val_loss: 22.3239 - lr: 5.0000e-05\n",
      "Epoch 35/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.6652 - val_loss: 22.4897 - lr: 5.0000e-05\n",
      "Epoch 36/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.7157 - val_loss: 22.3536 - lr: 5.0000e-05\n",
      "Epoch 37/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.6968 - val_loss: 22.3130 - lr: 5.0000e-05\n",
      "Epoch 38/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5794 - val_loss: 22.3703 - lr: 5.0000e-05\n",
      "Epoch 39/350\n",
      "10913/10913 [==============================] - 81s 7ms/step - loss: 24.6370 - val_loss: 22.2973 - lr: 5.0000e-05\n",
      "Epoch 40/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.6860 - val_loss: 22.3712 - lr: 5.0000e-05\n",
      "Epoch 41/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.6586 - val_loss: 22.3714 - lr: 5.0000e-05\n",
      "Epoch 42/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.6246 - val_loss: 22.5011 - lr: 5.0000e-05\n",
      "Epoch 43/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5587 - val_loss: 22.4249 - lr: 5.0000e-05\n",
      "Epoch 44/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.6597 - val_loss: 22.3643 - lr: 5.0000e-05\n",
      "Epoch 45/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.6127 - val_loss: 22.3724 - lr: 5.0000e-05\n",
      "Epoch 46/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.6509 - val_loss: 22.4412 - lr: 5.0000e-05\n",
      "Epoch 47/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.6259 - val_loss: 22.3188 - lr: 5.0000e-05\n",
      "Epoch 48/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.6181 - val_loss: 22.3952 - lr: 5.0000e-05\n",
      "Epoch 49/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.6236 - val_loss: 22.4452 - lr: 5.0000e-05\n",
      "Epoch 50/350\n",
      "10913/10913 [==============================] - 81s 7ms/step - loss: 24.5548 - val_loss: 22.3426 - lr: 1.0000e-05\n",
      "Epoch 51/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.6150 - val_loss: 22.3174 - lr: 1.0000e-05\n",
      "Epoch 52/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5667 - val_loss: 22.3447 - lr: 1.0000e-05\n",
      "Epoch 53/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5474 - val_loss: 22.3596 - lr: 1.0000e-05\n",
      "Epoch 54/350\n",
      "10913/10913 [==============================] - 82s 8ms/step - loss: 24.5597 - val_loss: 22.2813 - lr: 1.0000e-05\n",
      "Epoch 55/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5210 - val_loss: 22.3455 - lr: 1.0000e-05\n",
      "Epoch 56/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5258 - val_loss: 22.3698 - lr: 1.0000e-05\n",
      "Epoch 57/350\n",
      "10913/10913 [==============================] - 79s 7ms/step - loss: 24.5245 - val_loss: 22.3132 - lr: 1.0000e-05\n",
      "Epoch 58/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4957 - val_loss: 22.3283 - lr: 1.0000e-05\n",
      "Epoch 59/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5461 - val_loss: 22.3367 - lr: 1.0000e-05\n",
      "Epoch 60/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5148 - val_loss: 22.2898 - lr: 1.0000e-05\n",
      "Epoch 61/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.6185 - val_loss: 22.3066 - lr: 1.0000e-05\n",
      "Epoch 62/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5438 - val_loss: 22.3462 - lr: 1.0000e-05\n",
      "Epoch 63/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5498 - val_loss: 22.3182 - lr: 1.0000e-05\n",
      "Epoch 64/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5230 - val_loss: 22.3225 - lr: 1.0000e-05\n",
      "Epoch 65/350\n",
      "10913/10913 [==============================] - 79s 7ms/step - loss: 24.5459 - val_loss: 22.3042 - lr: 1.0000e-05\n",
      "Epoch 66/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5326 - val_loss: 22.3071 - lr: 1.0000e-05\n",
      "Epoch 67/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5269 - val_loss: 22.2991 - lr: 1.0000e-05\n",
      "Epoch 68/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5520 - val_loss: 22.2864 - lr: 1.0000e-05\n",
      "Epoch 69/350\n",
      "10913/10913 [==============================] - 82s 7ms/step - loss: 24.5230 - val_loss: 22.2806 - lr: 1.0000e-05\n",
      "Epoch 70/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4234 - val_loss: 22.3154 - lr: 1.0000e-05\n",
      "Epoch 71/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5034 - val_loss: 22.3087 - lr: 1.0000e-05\n",
      "Epoch 72/350\n",
      "10913/10913 [==============================] - 79s 7ms/step - loss: 24.5214 - val_loss: 22.3023 - lr: 1.0000e-05\n",
      "Epoch 73/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4715 - val_loss: 22.3073 - lr: 1.0000e-05\n",
      "Epoch 74/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5011 - val_loss: 22.2711 - lr: 1.0000e-05\n",
      "Epoch 75/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5026 - val_loss: 22.3021 - lr: 1.0000e-05\n",
      "Epoch 76/350\n",
      "10913/10913 [==============================] - 79s 7ms/step - loss: 24.4541 - val_loss: 22.3292 - lr: 1.0000e-05\n",
      "Epoch 77/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4975 - val_loss: 22.3197 - lr: 1.0000e-05\n",
      "Epoch 78/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5042 - val_loss: 22.3335 - lr: 1.0000e-05\n",
      "Epoch 79/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4629 - val_loss: 22.2932 - lr: 1.0000e-05\n",
      "Epoch 80/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5305 - val_loss: 22.3306 - lr: 1.0000e-05\n",
      "Epoch 81/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5165 - val_loss: 22.3448 - lr: 1.0000e-05\n",
      "Epoch 82/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4933 - val_loss: 22.3027 - lr: 1.0000e-05\n",
      "Epoch 83/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4956 - val_loss: 22.3186 - lr: 1.0000e-05\n",
      "Epoch 84/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5416 - val_loss: 22.3313 - lr: 1.0000e-05\n",
      "Epoch 85/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4824 - val_loss: 22.3132 - lr: 1.0000e-05\n",
      "Epoch 86/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4743 - val_loss: 22.2763 - lr: 1.0000e-05\n",
      "Epoch 87/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4763 - val_loss: 22.2914 - lr: 1.0000e-05\n",
      "Epoch 88/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5179 - val_loss: 22.3123 - lr: 1.0000e-05\n",
      "Epoch 89/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4622 - val_loss: 22.3401 - lr: 1.0000e-05\n",
      "Epoch 90/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5398 - val_loss: 22.3296 - lr: 1.0000e-05\n",
      "Epoch 91/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5191 - val_loss: 22.2716 - lr: 1.0000e-05\n",
      "Epoch 92/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4758 - val_loss: 22.3581 - lr: 1.0000e-05\n",
      "Epoch 93/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4880 - val_loss: 22.2523 - lr: 1.0000e-05\n",
      "Epoch 94/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4898 - val_loss: 22.3142 - lr: 1.0000e-05\n",
      "Epoch 95/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5198 - val_loss: 22.3113 - lr: 1.0000e-05\n",
      "Epoch 96/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5206 - val_loss: 22.3419 - lr: 1.0000e-05\n",
      "Epoch 97/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5403 - val_loss: 22.3223 - lr: 1.0000e-05\n",
      "Epoch 98/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5054 - val_loss: 22.3008 - lr: 1.0000e-05\n",
      "Epoch 99/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4636 - val_loss: 22.3158 - lr: 1.0000e-05\n",
      "Epoch 100/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4933 - val_loss: 22.2899 - lr: 1.0000e-05\n",
      "Epoch 101/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4719 - val_loss: 22.3148 - lr: 1.0000e-05\n",
      "Epoch 102/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5201 - val_loss: 22.2922 - lr: 1.0000e-05\n",
      "Epoch 103/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5110 - val_loss: 22.3214 - lr: 1.0000e-05\n",
      "Epoch 104/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4388 - val_loss: 22.3110 - lr: 1.0000e-05\n",
      "Epoch 105/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5175 - val_loss: 22.3257 - lr: 1.0000e-05\n",
      "Epoch 106/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4878 - val_loss: 22.3121 - lr: 1.0000e-05\n",
      "Epoch 107/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4972 - val_loss: 22.2827 - lr: 1.0000e-05\n",
      "Epoch 108/350\n",
      "10913/10913 [==============================] - 79s 7ms/step - loss: 24.4739 - val_loss: 22.2843 - lr: 1.0000e-05\n",
      "Epoch 109/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4823 - val_loss: 22.2849 - lr: 1.0000e-05\n",
      "Epoch 110/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4821 - val_loss: 22.2960 - lr: 1.0000e-05\n",
      "Epoch 111/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4669 - val_loss: 22.3439 - lr: 1.0000e-05\n",
      "Epoch 112/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4768 - val_loss: 22.3264 - lr: 1.0000e-05\n",
      "Epoch 113/350\n",
      "10913/10913 [==============================] - 81s 7ms/step - loss: 24.4883 - val_loss: 22.3099 - lr: 1.0000e-05\n",
      "Epoch 114/350\n",
      "10913/10913 [==============================] - 82s 7ms/step - loss: 24.5054 - val_loss: 22.3583 - lr: 1.0000e-05\n",
      "Epoch 115/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4910 - val_loss: 22.2691 - lr: 1.0000e-05\n",
      "Epoch 116/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4991 - val_loss: 22.3467 - lr: 1.0000e-05\n",
      "Epoch 117/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4760 - val_loss: 22.3047 - lr: 1.0000e-05\n",
      "Epoch 118/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4436 - val_loss: 22.2645 - lr: 1.0000e-05\n",
      "Epoch 119/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4592 - val_loss: 22.2845 - lr: 1.0000e-05\n",
      "Epoch 120/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.5074 - val_loss: 22.2992 - lr: 1.0000e-05\n",
      "Epoch 121/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4847 - val_loss: 22.2722 - lr: 1.0000e-05\n",
      "Epoch 122/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4423 - val_loss: 22.3016 - lr: 1.0000e-05\n",
      "Epoch 123/350\n",
      "10913/10913 [==============================] - 80s 7ms/step - loss: 24.4454 - val_loss: 22.2628 - lr: 1.0000e-05\n",
      "7795/7795 [==============================] - 16s 2ms/step\n",
      "Test MSE: 22.320049879238173\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Bidirectional, Conv1D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('Coffee_Stores_Data.csv')\n",
    "\n",
    "# Preprocess and remove NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Convert 'BusinessDate' to a datetime index and perform feature engineering\n",
    "df['BusinessDate'] = pd.to_datetime(df['BusinessDate'])\n",
    "df.set_index('BusinessDate', inplace=True)\n",
    "df['Month'] = df.index.month\n",
    "df['Year'] = df.index.year\n",
    "df['Sales'] = df['SoldQuantity'] * 3\n",
    "\n",
    "# Selecting features and target\n",
    "selected_features = ['ReceivedQuantity', 'LatestOrder', 'StockedOut', 'Month', 'Year']\n",
    "X = df[selected_features]\n",
    "y = df['Sales']\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Define the LSTM model with advanced architecture\n",
    "inputs = Input(shape=(1, X_train.shape[2]))\n",
    "x = Conv1D(filters=32, kernel_size=1, activation='relu')(inputs)  # Conv1D layer for sequence feature extraction\n",
    "x = Bidirectional(LSTM(80, return_sequences=True, dropout=0.25))(x)\n",
    "x = Bidirectional(LSTM(40, return_sequences=False, dropout=0.25))(x)\n",
    "x = Dense(100, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(x)  # Regularization\n",
    "x = BatchNormalization()(x)  # Batch Normalization\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(1)(x)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer=Nadam(learning_rate=0.0005), loss='mean_squared_error')\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=0.00001)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=350, batch_size=64, validation_split=0.3, verbose=1, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Predictions and Evaluation\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Test MSE: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Bidirectional, Conv1D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('Coffee_Stores_Data.csv')\n",
    "\n",
    "# Preprocess and remove NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Convert 'BusinessDate' to a datetime index and perform feature engineering\n",
    "df['BusinessDate'] = pd.to_datetime(df['BusinessDate'])\n",
    "df.set_index('BusinessDate', inplace=True)\n",
    "df['Month'] = df.index.month\n",
    "df['Year'] = df.index.year\n",
    "df['Sales'] = df['SoldQuantity'] * 3\n",
    "\n",
    "# Selecting features and target\n",
    "selected_features = ['ReceivedQuantity', 'LatestOrder', 'StockedOut', 'Month', 'Year']\n",
    "X = df[selected_features]\n",
    "y = df['Sales']\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Define the LSTM model with advanced architecture\n",
    "inputs = Input(shape=(1, X_train.shape[2]))\n",
    "x = Conv1D(filters=32, kernel_size=1, activation='relu')(inputs)  # Conv1D layer for sequence feature extraction\n",
    "x = Bidirectional(LSTM(80, return_sequences=True, dropout=0.25))(x)\n",
    "x = Bidirectional(LSTM(40, return_sequences=False, dropout=0.25))(x)\n",
    "x = Dense(100, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(x)  # Regularization\n",
    "x = BatchNormalization()(x)  # Batch Normalization\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(1)(x)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer=Nadam(learning_rate=0.0005), loss='mean_squared_error')\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=0.00001)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=350, batch_size=64, validation_split=0.3, verbose=1, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Predictions and Evaluation\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Test MSE: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_array = X_test.reshape(-1, len(selected_features))\n",
    "\n",
    "# Convert y_test to a NumPy array and flatten if needed\n",
    "y_test_array = np.array(y_test).flatten()\n",
    "\n",
    "# Create a DataFrame with true values, predicted values, and selected features\n",
    "result_df = pd.DataFrame(X_test_array, columns=selected_features)\n",
    "result_df['True Sales'] = y_test_array\n",
    "result_df['Predicted Sales'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReceivedQuantity</th>\n",
       "      <th>LatestOrder</th>\n",
       "      <th>StockedOut</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>True Sales</th>\n",
       "      <th>Predicted Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.188119</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>34.346584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.138614</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.761110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.574140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.206759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.128713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249419</th>\n",
       "      <td>0.158416</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.932339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249420</th>\n",
       "      <td>0.133663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.229206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249421</th>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.711812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249422</th>\n",
       "      <td>0.128713</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.905665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249423</th>\n",
       "      <td>0.217822</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>33.335789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249424 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ReceivedQuantity  LatestOrder  StockedOut     Month  Year  True Sales  \\\n",
       "0               0.188119     0.608696         1.0  0.818182   0.0        33.0   \n",
       "1               0.138614     0.391304         0.0  0.090909   1.0         3.0   \n",
       "2               0.168317     0.652174         0.0  0.090909   1.0        12.0   \n",
       "3               0.148515     0.521739         0.0  0.909091   0.0         6.0   \n",
       "4               0.128713     0.000000         0.0  0.818182   0.0         0.0   \n",
       "...                  ...          ...         ...       ...   ...         ...   \n",
       "249419          0.158416     0.478261         1.0  0.545455   0.0        18.0   \n",
       "249420          0.133663     0.000000         0.0  0.636364   0.0         0.0   \n",
       "249421          0.168317     0.782609         0.0  0.636364   0.0        15.0   \n",
       "249422          0.128713     0.521739         1.0  0.272727   1.0         3.0   \n",
       "249423          0.217822     0.782609         0.0  0.727273   0.0        27.0   \n",
       "\n",
       "        Predicted Sales  \n",
       "0             34.346584  \n",
       "1              4.761110  \n",
       "2             15.574140  \n",
       "3              7.206759  \n",
       "4              0.103714  \n",
       "...                 ...  \n",
       "249419        16.932339  \n",
       "249420         0.229206  \n",
       "249421        15.711812  \n",
       "249422         5.905665  \n",
       "249423        33.335789  \n",
       "\n",
       "[249424 rows x 7 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Complexity: The increased complexity of the model might not be necessary or beneficial for the given dataset. Sometimes, simpler models perform better on certain types of data.\n",
    "\n",
    "Overfitting/Underfitting Balance: While efforts were made to prevent overfitting (like increased dropout and regularization), it's possible that the balance between bias and variance was not optimally achieved.\n",
    "\n",
    "Training Dynamics: The increased number of epochs and changes in batch size and learning rate might have influenced the training process, but not necessarily in a way that enhances performance for this specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tunning by regularization of Input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "5457/5457 [==============================] - 85s 14ms/step - loss: 31.4948 - val_loss: 23.5189 - lr: 5.0000e-04\n",
      "Epoch 2/35\n",
      "5457/5457 [==============================] - 78s 14ms/step - loss: 25.3353 - val_loss: 23.0020 - lr: 5.0000e-04\n",
      "Epoch 3/35\n",
      "5457/5457 [==============================] - 77s 14ms/step - loss: 25.0205 - val_loss: 22.8409 - lr: 5.0000e-04\n",
      "Epoch 4/35\n",
      "5457/5457 [==============================] - 78s 14ms/step - loss: 24.8534 - val_loss: 22.8669 - lr: 5.0000e-04\n",
      "Epoch 5/35\n",
      "5457/5457 [==============================] - 77s 14ms/step - loss: 24.6445 - val_loss: 22.6513 - lr: 5.0000e-04\n",
      "Epoch 6/35\n",
      "5457/5457 [==============================] - 78s 14ms/step - loss: 24.4780 - val_loss: 22.5557 - lr: 5.0000e-04\n",
      "Epoch 7/35\n",
      "5457/5457 [==============================] - 77s 14ms/step - loss: 24.4073 - val_loss: 23.2719 - lr: 5.0000e-04\n",
      "Epoch 8/35\n",
      "5457/5457 [==============================] - 80s 15ms/step - loss: 24.2632 - val_loss: 22.9199 - lr: 5.0000e-04\n",
      "Epoch 9/35\n",
      "5457/5457 [==============================] - 77s 14ms/step - loss: 24.1370 - val_loss: 22.4434 - lr: 5.0000e-04\n",
      "Epoch 10/35\n",
      "5457/5457 [==============================] - 76s 14ms/step - loss: 24.1196 - val_loss: 23.1072 - lr: 5.0000e-04\n",
      "Epoch 11/35\n",
      "5457/5457 [==============================] - 76s 14ms/step - loss: 24.0188 - val_loss: 22.4850 - lr: 5.0000e-04\n",
      "Epoch 12/35\n",
      "5457/5457 [==============================] - 75s 14ms/step - loss: 23.9993 - val_loss: 22.3621 - lr: 5.0000e-04\n",
      "Epoch 13/35\n",
      "5457/5457 [==============================] - 76s 14ms/step - loss: 23.9071 - val_loss: 23.0156 - lr: 5.0000e-04\n",
      "Epoch 14/35\n",
      "5457/5457 [==============================] - 78s 14ms/step - loss: 23.9090 - val_loss: 22.5936 - lr: 5.0000e-04\n",
      "Epoch 15/35\n",
      "5457/5457 [==============================] - 79s 14ms/step - loss: 23.8592 - val_loss: 22.3770 - lr: 5.0000e-04\n",
      "Epoch 16/35\n",
      "5457/5457 [==============================] - 78s 14ms/step - loss: 23.8130 - val_loss: 22.8170 - lr: 5.0000e-04\n",
      "Epoch 17/35\n",
      "5457/5457 [==============================] - 79s 14ms/step - loss: 23.8238 - val_loss: 22.5109 - lr: 5.0000e-04\n",
      "Epoch 18/35\n",
      "5457/5457 [==============================] - 77s 14ms/step - loss: 23.7919 - val_loss: 22.5782 - lr: 5.0000e-04\n",
      "Epoch 19/35\n",
      "5457/5457 [==============================] - 77s 14ms/step - loss: 23.6687 - val_loss: 22.5229 - lr: 5.0000e-04\n",
      "Epoch 20/35\n",
      "5457/5457 [==============================] - 77s 14ms/step - loss: 23.7396 - val_loss: 22.2516 - lr: 5.0000e-04\n",
      "Epoch 21/35\n",
      "5457/5457 [==============================] - 74s 14ms/step - loss: 23.6542 - val_loss: 22.5833 - lr: 5.0000e-04\n",
      "Epoch 22/35\n",
      "5457/5457 [==============================] - 75s 14ms/step - loss: 23.6645 - val_loss: 22.5400 - lr: 5.0000e-04\n",
      "Epoch 23/35\n",
      "5457/5457 [==============================] - 76s 14ms/step - loss: 23.6648 - val_loss: 22.5910 - lr: 5.0000e-04\n",
      "Epoch 24/35\n",
      "5457/5457 [==============================] - 78s 14ms/step - loss: 23.6327 - val_loss: 22.7769 - lr: 5.0000e-04\n",
      "Epoch 25/35\n",
      "5457/5457 [==============================] - 78s 14ms/step - loss: 23.6055 - val_loss: 22.4778 - lr: 5.0000e-04\n",
      "Epoch 26/35\n",
      "5457/5457 [==============================] - 77s 14ms/step - loss: 23.6370 - val_loss: 22.3779 - lr: 5.0000e-04\n",
      "Epoch 27/35\n",
      "5457/5457 [==============================] - 80s 15ms/step - loss: 23.5090 - val_loss: 22.2127 - lr: 5.0000e-04\n",
      "Epoch 28/35\n",
      "5457/5457 [==============================] - 78s 14ms/step - loss: 23.5529 - val_loss: 22.3619 - lr: 5.0000e-04\n",
      "Epoch 29/35\n",
      "5457/5457 [==============================] - 78s 14ms/step - loss: 23.5189 - val_loss: 22.1892 - lr: 5.0000e-04\n",
      "Epoch 30/35\n",
      "5457/5457 [==============================] - 77s 14ms/step - loss: 23.5247 - val_loss: 22.6003 - lr: 5.0000e-04\n",
      "Epoch 31/35\n",
      "5457/5457 [==============================] - 78s 14ms/step - loss: 23.4604 - val_loss: 22.3790 - lr: 5.0000e-04\n",
      "Epoch 32/35\n",
      "5457/5457 [==============================] - 77s 14ms/step - loss: 23.4541 - val_loss: 22.5578 - lr: 5.0000e-04\n",
      "Epoch 33/35\n",
      "5457/5457 [==============================] - 77s 14ms/step - loss: 23.4274 - val_loss: 22.4525 - lr: 5.0000e-04\n",
      "Epoch 34/35\n",
      "5457/5457 [==============================] - 77s 14ms/step - loss: 23.4226 - val_loss: 22.7623 - lr: 5.0000e-04\n",
      "Epoch 35/35\n",
      "5457/5457 [==============================] - 74s 14ms/step - loss: 23.3952 - val_loss: 22.1402 - lr: 5.0000e-04\n",
      "7795/7795 [==============================] - 32s 4ms/step\n",
      "Test MSE: 22.181716991798567\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler  # Import StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Bidirectional, Conv1D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('Coffee_Stores_Data.csv')\n",
    "\n",
    "# Preprocess and remove NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Convert 'BusinessDate' to a datetime index and perform feature engineering\n",
    "df['BusinessDate'] = pd.to_datetime(df['BusinessDate'])\n",
    "df.set_index('BusinessDate', inplace=True)\n",
    "df['Month'] = df.index.month\n",
    "df['Year'] = df.index.year\n",
    "df['Sales'] = df['SoldQuantity'] * 3\n",
    "\n",
    "# Selecting features and target\n",
    "selected_features = ['ReceivedQuantity', 'LatestOrder', 'StockedOut', 'Month', 'Year']\n",
    "X = df[selected_features]\n",
    "y = df['Sales']\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the selected features\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Define the LSTM model with advanced architecture\n",
    "inputs = Input(shape=(1, X_train.shape[2]))\n",
    "x = Conv1D(filters=32, kernel_size=1, activation='relu')(inputs)  # Conv1D layer for sequence feature extraction\n",
    "x = Bidirectional(LSTM(80, return_sequences=True, dropout=0.25))(x)\n",
    "x = Bidirectional(LSTM(40, return_sequences=False, dropout=0.25))(x)\n",
    "x = Dense(100, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(x)  # Regularization\n",
    "x = BatchNormalization()(x)  # Batch Normalization\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(1)(x)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer=Nadam(learning_rate=0.0005), loss='mean_squared_error')\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=0.00001)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=35, batch_size=128, validation_split=0.3, verbose=1, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Predictions and Evaluation\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Test MSE: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
